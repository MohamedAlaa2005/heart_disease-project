{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16606132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9735f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease=pd.read_csv(r\"Data\\heart_disease_selected_feature.csv\")\n",
    "#drop column unname0\n",
    "heart_disease=heart_disease.iloc[:,1:]\n",
    "# spilt data to feature and target\n",
    "x=heart_disease.iloc[:,:-1]\n",
    "y=heart_disease['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "560e0194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (242, 6), Test shape: (61, 6)\n",
      "\n",
      "üî∑ Logistic Regression\n",
      "‚úÖ Best Params: {'C': 10, 'class_weight': None, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "‚úÖ Test Accuracy: 0.6229508196721312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        33\n",
      "           1       0.44      0.36      0.40        11\n",
      "           2       0.25      0.14      0.18         7\n",
      "           3       0.29      0.29      0.29         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.36      0.35      0.35        61\n",
      "weighted avg       0.57      0.62      0.59        61\n",
      "\n",
      "\n",
      "üî∑ Decision Tree\n",
      "‚úÖ Best Params: {'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "‚úÖ Test Accuracy: 0.6229508196721312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82        33\n",
      "           1       0.38      0.55      0.44        11\n",
      "           2       0.20      0.14      0.17         7\n",
      "           3       0.75      0.43      0.55         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.42      0.39      0.40        61\n",
      "weighted avg       0.61      0.62      0.61        61\n",
      "\n",
      "\n",
      "üî∑ SVM (RBF)\n",
      "‚úÖ Best Params: {'C': 1, 'gamma': 'auto', 'kernel': 'rbf', 'probability': True, 'shrinking': True}\n",
      "‚úÖ Test Accuracy: 0.6065573770491803\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87        33\n",
      "           1       0.17      0.18      0.17        11\n",
      "           2       0.29      0.29      0.29         7\n",
      "           3       0.50      0.29      0.36         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.61        61\n",
      "   macro avg       0.35      0.34      0.34        61\n",
      "weighted avg       0.56      0.61      0.58        61\n",
      "\n",
      "\n",
      "üî∑ Random Forest\n",
      "‚úÖ Best Params: {'bootstrap': True, 'criterion': 'gini', 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "‚úÖ Test Accuracy: 0.5737704918032787\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85        33\n",
      "           1       0.20      0.18      0.19        11\n",
      "           2       0.14      0.14      0.14         7\n",
      "           3       0.33      0.14      0.20         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.57        61\n",
      "   macro avg       0.29      0.28      0.28        61\n",
      "weighted avg       0.51      0.57      0.53        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "param_logreg = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'penalty': ['l2'],\n",
    "    'max_iter': [100, 200],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "param_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': [None, 'sqrt']\n",
    "}\n",
    "\n",
    "param_svm = {\n",
    "    'C': [0.1, 1, 10,0],\n",
    "    'gamma': ['scale', 'auto', 0.1],\n",
    "    'kernel': ['rbf'],\n",
    "    'shrinking': [True, False],\n",
    "    'probability': [True]\n",
    "}\n",
    "\n",
    "param_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# 3Ô∏è‚É£ Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": (LogisticRegression(), param_logreg),\n",
    "    \"Decision Tree\": (DecisionTreeClassifier(), param_dt),\n",
    "    \"SVM (RBF)\": (SVC(), param_svm),\n",
    "    \"Random Forest\": (RandomForestClassifier(), param_rf)\n",
    "}\n",
    "\n",
    "for name, (model, params) in models.items():\n",
    "    print(f\"\\nüî∑ {name}\")\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid = GridSearchCV(model, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = grid.best_params_\n",
    "\n",
    "\n",
    "    # 3Ô∏è‚É£ Re-train a NEW model using best params on the **full training set**\n",
    "    model_best = type(grid.estimator)(**best_params)\n",
    "    model_best.fit(X_train, y_train)\n",
    "    y_pred=model_best.predict(X_test)\n",
    "    print(\"‚úÖ Best Params:\", grid.best_params_)\n",
    "    print(\"‚úÖ Test Accuracy:\", model_best.score(X_test, y_test))\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64a79d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Baseline Dummy Classifier (Most Frequent)\n",
      "Accuracy: 0.5409836065573771\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      1.00      0.70        33\n",
      "           1       0.00      0.00      0.00        11\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.54        61\n",
      "   macro avg       0.11      0.20      0.14        61\n",
      "weighted avg       0.29      0.54      0.38        61\n",
      "\n",
      "üîµ Logistic Regression (Default)\n",
      "Accuracy: 0.6229508196721312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86        33\n",
      "           1       0.40      0.36      0.38        11\n",
      "           2       0.25      0.14      0.18         7\n",
      "           3       0.38      0.43      0.40         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.37      0.37      0.36        61\n",
      "weighted avg       0.58      0.62      0.60        61\n",
      "\n",
      "üü¢ Logistic Regression (Best Model)\n",
      "Accuracy: 0.6229508196721312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        33\n",
      "           1       0.44      0.36      0.40        11\n",
      "           2       0.25      0.14      0.18         7\n",
      "           3       0.29      0.29      0.29         7\n",
      "           4       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.62        61\n",
      "   macro avg       0.36      0.35      0.35        61\n",
      "weighted avg       0.57      0.62      0.59        61\n",
      "\n",
      "üìà Model improvement over Dummy: 0.0820 (from 0.5410 to 0.6230)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 1Ô∏è‚É£ Train Dummy Classifier (Baseline)\n",
    "dummy = DummyClassifier(strategy='most_frequent')  \n",
    "dummy.fit(X_train, y_train)\n",
    "y_dummy_pred = dummy.predict(X_test)\n",
    "dummy_acc = accuracy_score(y_test, y_dummy_pred)\n",
    "\n",
    "print(\"üî¥ Baseline Dummy Classifier (Most Frequent)\")\n",
    "print(\"Accuracy:\", dummy_acc)\n",
    "print(classification_report(y_test, y_dummy_pred))\n",
    "\n",
    "# 2Ô∏è‚É£ Train Logistic Regression (Default)\n",
    "default_model = LogisticRegression()\n",
    "default_model.fit(X_train, y_train)\n",
    "y_default_pred = default_model.predict(X_test)\n",
    "default_acc = accuracy_score(y_test, y_default_pred)\n",
    "\n",
    "print(\"üîµ Logistic Regression (Default)\")\n",
    "print(\"Accuracy:\", default_acc)\n",
    "print(classification_report(y_test, y_default_pred))\n",
    "\n",
    "# 3Ô∏è‚É£ Train Logistic Regression (Best Params from GridSearch)\n",
    "best_params = grid.best_params_\n",
    "log_best = LogisticRegression(C= 10,  max_iter =100,penalty= 'l2' ,solver= 'liblinear')\n",
    "log_best.fit(X_train, y_train)\n",
    "y_best_pred = log_best.predict(X_test)\n",
    "best_acc = accuracy_score(y_test, y_best_pred)\n",
    "\n",
    "print(\"üü¢ Logistic Regression (Best Model)\")\n",
    "print(\"Accuracy:\", best_acc)\n",
    "print(classification_report(y_test, y_best_pred))\n",
    "\n",
    "# 4Ô∏è‚É£ Show improvement\n",
    "improvement = best_acc - dummy_acc\n",
    "print(f\"üìà Model improvement over Dummy: {improvement:.4f} (from {dummy_acc:.4f} to {best_acc:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
